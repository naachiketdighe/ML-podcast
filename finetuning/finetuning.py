# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A2IFgJY3Iep5cM6ANhVqQrjdvPHEDrWk
"""

!pip install -Uq pip --progress-bar off
!pip install -q transformers
!pip install -q datasets
!pip install -Uq bitsandbytes
!pip install -q trl
!pip install -q colored
!pip install -q accelerate
!pip install -q huggingface_hub       # install the client
!pip install shutil
!pip install -q google

# Commented out IPython magic to ensure Python compatibility.
import random
from textwrap import dedent
from typing import Dict, List

import matplotlib as mpl
import matplotlib.colors as colors
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import torch
from colored import Back, Fore, Style
from datasets import Dataset, load_dataset
from matplotlib.ticker import PercentFormatter
from peft import (
    LoraConfig,
    PeftModel,
    TaskType,
    get_peft_model,
    prepare_model_for_kbit_training,
)
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader
from tqdm import tqdm
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    pipeline,
)
from trl import DataCollatorForCompletionOnlyLM, SFTConfig, SFTTrainer
from google.colab import drive
import os
import shutil


# %matplotlib inline
# %config InlineBackend.figure_format='retina'

COLORS = ["#bae1ff", "#ffb3ba", "#ffdfba", "#ffffba", "#baffc9"]

sns.set(style="whitegrid", palette="muted", font_scale=1.2)
sns.set_palette(sns.color_palette(COLORS))

cmap = colors.LinearSegmentedColormap.from_list("custom_cmap", COLORS[:2])

MY_STYLE = {
    "figure.facecolor": "black",
    "axes.facecolor": "black",
    "axes.edgecolor": "white",
    "axes.labelcolor": "white",
    "axes.linewidth": 0.5,
    "text.color": "white",
    "xtick.color": "white",
    "ytick.color": "white",
    "grid.color": "gray",
    "grid.linestyle": "--",
    "grid.linewidth": 0.5,
    "axes.grid": True,
    "xtick.labelsize": "medium",
    "ytick.labelsize": "medium",
    "axes.titlesize": "large",
    "axes.labelsize": "large",
    "lines.color": COLORS[0],
    "patch.edgecolor": "white",
}

mpl.rcParams.update(MY_STYLE)

SEED = 42


def seed_everything(seed: int):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)


seed_everything(SEED)
PAD_TOKEN = "<|pad|>"
MODEL_NAME = "meta-llama/Meta-Llama-3.1-8B-Instruct"
NEW_MODEL = "Llama-3.1-8B-Instruct-Dating-instructor"

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True, bnb_4bit_quant_type="nf4", bnb_4bit_compute_dtype=torch.bfloat16
)

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)
tokenizer.add_special_tokens({"pad_token": PAD_TOKEN})
tokenizer.padding_side = "right"

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    quantization_config=quantization_config,
    #     attn_implementation="flash_attention_2",
    #     attn_implementation="sdpa",
    device_map="auto",
)
model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=8)

tokenizer.bos_token, tokenizer.bos_token_id

model

dataset = pd.read_csv('transcript_cleaned_output.csv')

dataset

def count_tokens(row: Dict) -> int:
    return len(
        tokenizer(
            row["text"],
            add_special_tokens=True,
            return_attention_mask=False,
        )["input_ids"]
    )

dataset['token_count'] = dataset.apply(count_tokens, axis=1)

dataset.head()

plt.hist(dataset.token_count, weights=np.ones(len(dataset.token_count)) / len(dataset.token_count))
plt.gca().yaxis.set_major_formatter(PercentFormatter(1))
plt.xlabel("Tokens")
plt.ylabel("Percentage")
plt.show();

len(dataset[dataset.token_count < 512]), len(dataset), len(dataset[dataset.token_count < 512]) / len(dataset)

df = dataset[dataset.token_count < 512]
df.shape

train, temp = train_test_split(df, test_size=0.2)
val, test = train_test_split(temp, test_size=0.2)

len(train) / len(df), len(val) / len(df), len(test) / len(df)

len(train), len(val), len(test)

train.sample(n=2020).to_json("train.json", orient="records", lines=True)
val.sample(n=404).to_json("val.json", orient="records", lines=True)
test.sample(n=100).to_json("test.json", orient="records", lines=True)

database = load_dataset(
    "json",
    data_files={"train": "train.json", "validation": "val.json", "test": "test.json"},
)

print(database["train"][0]["text"])

"""Testing the original model"""

# pipe = pipeline(
#     task="text-generation",
#     model=model,
#     tokenizer=tokenizer,
#     max_new_tokens=128,
#     return_full_text=False,
# )

# def create_test_prompt(data_row):
#     prompt = dedent(
#         f"""
#     {data_row["question"]}

#     Information:

#     ```
#     {data_row["context"]}
#     ```
#     """
#     )
#     messages = [
#         {
#             "role": "system",
#             "content": "You are Mark Singh, a dating coach who gives practical advice to young men with an unapologetic, witty, and playful banter style.",
#         },
#         {"role": "user", "content": prompt},
#     ]
#     return tokenizer.apply_chat_template(
#         messages, tokenize=False, add_generation_prompt=True
#     )

# row = database["test"][0]
# prompt = create_test_prompt(row)
# print(prompt)

# %%time
# outputs = pipe(prompt)
# response = f"""
# answer:     {row["answer"]}
# prediction: {outputs[0]["generated_text"]}
# """
# print(response)

"""Training only on the completions"""

response_template = "<|end_header_id|>"
collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)

examples = [database["train"][0]["text"]]
encodings = [tokenizer(e) for e in examples]

dataloader = DataLoader(encodings, collate_fn=collator, batch_size=1)

batch = next(iter(dataloader))
batch.keys()

batch["labels"]

"""LoRA setup"""

lora_config = LoraConfig(
    r=32,
    lora_alpha=16,
    target_modules=[
        "self_attn.q_proj",
        "self_attn.k_proj",
        "self_attn.v_proj",
        "self_attn.o_proj",
        "mlp.gate_proj",
        "mlp.up_proj",
        "mlp.down_proj",
    ],
    lora_dropout=0.05,
    bias="none",
    task_type=TaskType.CAUSAL_LM,
)
model = prepare_model_for_kbit_training(model)
model = get_peft_model(model, lora_config)

model.print_trainable_parameters()

"""Training"""

OUTPUT_DIR = "training"

tokenizer.eos_token, tokenizer.eos_token_id

sft_config = SFTConfig(
    output_dir=OUTPUT_DIR,
    dataset_text_field="text",
    max_seq_length=512,
    num_train_epochs=1,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    gradient_accumulation_steps=4,
    optim="paged_adamw_8bit",
    eval_strategy="steps",
    eval_steps=0.2,
    save_steps=0.2,
    logging_steps=10,
    learning_rate=1e-4,
    fp16=True,  # or bf16=True,
    save_strategy="steps",
    warmup_ratio=0.1,
    save_total_limit=2,
    lr_scheduler_type="constant",
    report_to="tensorboard",
    save_safetensors=True,
    dataset_kwargs={
        "add_special_tokens": False,  # We template with special tokens
        "append_concat_token": False,  # No need to add additional separator token
    },
    seed=SEED,
)

trainer = SFTTrainer(
    model=model,
    args=sft_config,
    train_dataset=database["train"],
    eval_dataset=database["validation"],
    data_collator=collator,
)

trainer.train()

# Simple Google Drive Save with Verification

# 1. Mount Google Drive
drive.mount('/content/drive')

# 2. Quick verification it's really Google Drive
if not os.path.exists('/content/drive/MyDrive'):
    print("❌ Google Drive not mounted properly!")
    exit()

# Quick test write to confirm it's Google Drive
test_file = '/content/drive/MyDrive/colab_test.txt'
with open(test_file, 'w') as f:
    f.write('test')
os.remove(test_file)
print("✅ Google Drive confirmed accessible")

# 3. Save model to Google Drive
local_path = NEW_MODEL  # Your model path
drive_path = "/content/drive/MyDrive/llama3b_lora_finetuned"

# Save
trainer.save_model(local_path)
tokenizer.save_pretrained(local_path)

# Copy to Google Drive
if os.path.exists(drive_path):
    shutil.rmtree(drive_path)

shutil.copytree(local_path, drive_path)
os.system('sync')

# 4. Simple verification
if os.path.exists(drive_path) and len(os.listdir(drive_path)) > 0:
    print(f"✅ SUCCESS: Model saved to Google Drive at {drive_path}")
else:
    print("❌ FAILED: Model not saved to Google Drive")

print("Done!")

# trainer.save_model(NEW_MODEL)
# print("Model saved locally to:", OUTPUT_DIR)

# # Save LoRA adapters specifically (this is what you really need)
# # model.save_pretrained(OUTPUT_DIR)
# tokenizer.save_pretrained(OUTPUT_DIR)

# # Copy to Google Drive for permanent storage
# import shutil
# drive_save_path = "/content/drive/MyDrive/llama3b_lora_finetuned"
# try:
#     shutil.copytree(OUTPUT_DIR, drive_save_path, dirs_exist_ok=True)
#     print(f"Model successfully backed up to Google Drive: {drive_save_path}")
# except Exception as e:
#     print(f"Error saving to Drive: {e}")

del trainer
import gc
gc.collect()

torch.cuda.empty_cache()

!nvidia-smi

# Simple LoRA Model Reload from Google Drive
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import torch

# Clear memory first
torch.cuda.empty_cache()

# Path to your saved model
drive_path = "/content/drive/MyDrive/llama3b_lora_finetuned"

# Your base model name (replace with the actual base model you used)
base_model_name = "meta-llama/Meta-Llama-3.1-8B-Instruct"  # Replace with your base model

print("Loading base model...")
# Load base model

base_model = AutoModelForCausalLM.from_pretrained(
    base_model_name,
    torch_dtype=torch.float16,
    device_map="auto",
)

print("Loading tokenizer...")
# Load tokenizer from saved path
tokenizer = AutoTokenizer.from_pretrained(drive_path)

base_model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=8)

print("Loading LoRA adapter...")
# Load LoRA adapter
model = PeftModel.from_pretrained(base_model, drive_path)

print("Merging adapter...")
# Merge and unload for faster inference
model = model.merge_and_unload()

print("✅ Model successfully reloaded from Google Drive!")

# Quick test
def test_model(prompt="Hello, how are you?"):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=50,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id
        )

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"Input: {prompt}")
    print(f"Output: {response}")

# Test the reloaded model
test_model("what should you do if a girl does not text you back")

# model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=8)
# model = PeftModel.from_pretrained(model, NEW_MODEL)
# model = model.merge_and_unload()



model.push_to_hub("llama3b_lora_finetuned_dating_coach", tokenizer=tokenizer, max_shard_size="5GB")

# tokenizer.push_to_hub(NEW_MODEL)

